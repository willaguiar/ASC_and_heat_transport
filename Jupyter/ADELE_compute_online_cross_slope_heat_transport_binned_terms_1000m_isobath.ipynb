{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Antarctic, depth integrated, cross slope heat transport, online terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import cosima_cookbook as cc\n",
    "from cosima_cookbook import distributed as ccd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import glob,os\n",
    "import cmocean.cm as cmocean\n",
    "\n",
    "import logging\n",
    "logging.captureWarnings(True)\n",
    "logging.getLogger('py.warnings').setLevel(logging.ERROR)\n",
    "\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:43239</li>\n",
       "  <li><b>Dashboard: </b><a href='/proxy/34417/status' target='_blank'>/proxy/34417/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>7</li>\n",
       "  <li><b>Cores: </b>28</li>\n",
       "  <li><b>Memory: </b>137.44 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:43239' processes=7 threads=28, memory=137.44 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#client = Client(n_workers=4)\n",
    "# >> dask-scheduler\n",
    "# >> dask-worker tcp://10.0.64.9:8786 --memory-limit 4e9 --nprocs 6 --nthreads 1 --local-directory /local/g40/amh157\n",
    "#client = Client('tcp://10.0.64.9:8786', local_dir='/local/g40/amh157')\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = cc.database.create_session('/g/data/v45/akm157/jupyter_scripts/tides/ryf9091_tides.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2095'\n",
    "exp = '01deg_jra55v13_ryf9091'\n",
    "#exp = '01deg_jra55v13_ryf9091_tides_fixed'\n",
    "\n",
    "start_time='2095-01-01'\n",
    "end_time='2095-12-31'\n",
    "time_period = str(int(start_time[:4]))+'-'+str(int(end_time[:4]))\n",
    "\n",
    "# reference density value:\n",
    "rho_0 = 1035.0\n",
    "# specific heat capacity of sea water:\n",
    "cp = 3992.1\n",
    "lat_range = slice(-90,-59)\n",
    "\n",
    "isobath_depth = 1000\n",
    "\n",
    "# pick a freezing point temp:\n",
    "temp_freezing = -2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open contour data, extract lat/lon on contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = '/g/data/v45/akm157/model_data/access-om2/Antarctic_slope_contour_'+str(isobath_depth)+'m.npz'\n",
    "data = np.load(outfile)\n",
    "mask_y_transport = data['mask_y_transport']\n",
    "mask_x_transport = data['mask_x_transport']\n",
    "mask_y_transport_numbered = data['mask_y_transport_numbered']\n",
    "mask_x_transport_numbered = data['mask_x_transport_numbered']\n",
    "\n",
    "yt_ocean = cc.querying.getvar(exp,'yt_ocean',session,n=1)\n",
    "yt_ocean = yt_ocean.sel(yt_ocean=lat_range)\n",
    "yu_ocean = cc.querying.getvar(exp,'yu_ocean',session,n=1)\n",
    "yu_ocean = yu_ocean.sel(yu_ocean=lat_range)\n",
    "xt_ocean = cc.querying.getvar(exp,'xt_ocean',session,n=1)\n",
    "xu_ocean = cc.querying.getvar(exp,'xu_ocean',session,n=1)\n",
    "\n",
    "# convert isobath masks to data arrays, so we can multiply them later:\n",
    "mask_x_transport = xr.DataArray(mask_x_transport, coords = [('yt_ocean', yt_ocean), ('xu_ocean', xu_ocean)])\n",
    "mask_y_transport = xr.DataArray(mask_y_transport, coords = [('yu_ocean', yu_ocean), ('xt_ocean', xt_ocean)])\n",
    "mask_x_transport_numbered = xr.DataArray(mask_x_transport_numbered, coords = [('yt_ocean', yt_ocean), ('xt_ocean', xt_ocean)])\n",
    "mask_y_transport_numbered = xr.DataArray(mask_y_transport_numbered, coords = [('yt_ocean', yt_ocean), ('xt_ocean', xt_ocean)])\n",
    "\n",
    "num_points = int(np.maximum(np.max(mask_y_transport_numbered),np.max(mask_x_transport_numbered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_along_contour = np.zeros((num_points))\n",
    "lon_along_contour = np.zeros((num_points))\n",
    "\n",
    "# locations for zonal transport:\n",
    "x_indices_masked = mask_x_transport_numbered.stack().values\n",
    "x_indices = np.sort(x_indices_masked[x_indices_masked>0])\n",
    "for count in x_indices:\n",
    "    count = int(count)\n",
    "    jj = int(np.where(mask_x_transport_numbered==count)[0])\n",
    "    ii = int(np.where(mask_x_transport_numbered==count)[1])   \n",
    "    lon_along_contour[count-1] = xu_ocean[ii].values\n",
    "    lat_along_contour[count-1] = mask_x_transport_numbered.yt_ocean[jj].values\n",
    "    \n",
    "# locations for meridional transport:\n",
    "y_indices_masked = mask_y_transport_numbered.stack().values\n",
    "y_indices = np.sort(y_indices_masked[y_indices_masked>0])\n",
    "for count in y_indices:\n",
    "    count = int(count)\n",
    "    jj = np.where(mask_y_transport_numbered==count)[0]\n",
    "    ii = np.where(mask_y_transport_numbered==count)[1]\n",
    "    lon_along_contour[count-1] = mask_x_transport_numbered.xt_ocean[ii].values\n",
    "    lat_along_contour[count-1] = yu_ocean[jj].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-66.00806249, -66.00806249, -66.00806249, ..., -66.05030184,\n",
       "       -66.05030184, -66.02918216])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_along_contour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute heat transports calculated online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception during reset or similar\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data3/hh5/public/apps/miniconda3/envs/analysis3-20.04/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 693, in _finalize_fairy\n",
      "    fairy._reset(pool)\n",
      "  File \"/g/data3/hh5/public/apps/miniconda3/envs/analysis3-20.04/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 880, in _reset\n",
      "    pool._dialect.do_rollback(self)\n",
      "  File \"/g/data3/hh5/public/apps/miniconda3/envs/analysis3-20.04/lib/python3.7/site-packages/sqlalchemy/engine/default.py\", line 540, in do_rollback\n",
      "    dbapi_connection.rollback()\n",
      "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 23453619058496 and this is thread id 23450154944256.\n",
      "Exception closing connection <sqlite3.Connection object at 0x15540694d490>\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data3/hh5/public/apps/miniconda3/envs/analysis3-20.04/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 693, in _finalize_fairy\n",
      "    fairy._reset(pool)\n",
      "  File \"/g/data3/hh5/public/apps/miniconda3/envs/analysis3-20.04/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 880, in _reset\n",
      "    pool._dialect.do_rollback(self)\n",
      "  File \"/g/data3/hh5/public/apps/miniconda3/envs/analysis3-20.04/lib/python3.7/site-packages/sqlalchemy/engine/default.py\", line 540, in do_rollback\n",
      "    dbapi_connection.rollback()\n",
      "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 23453619058496 and this is thread id 23450154944256.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data3/hh5/public/apps/miniconda3/envs/analysis3-20.04/lib/python3.7/site-packages/sqlalchemy/pool/base.py\", line 270, in _close_connection\n",
      "    self._dialect.do_close(connection)\n",
      "  File \"/g/data3/hh5/public/apps/miniconda3/envs/analysis3-20.04/lib/python3.7/site-packages/sqlalchemy/engine/default.py\", line 546, in do_close\n",
      "    dbapi_connection.close()\n",
      "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 23453619058496 and this is thread id 23450154944256.\n"
     ]
    }
   ],
   "source": [
    "# Note temp_yflux_adv is also positioned on north centre edge of t-cell.\n",
    "# temp_yflux_adv = cp*rho*dzt*dxt*v*temp\n",
    "\n",
    "temp_yflux = cc.querying.getvar(exp,'temp_yflux_adv',session,start_time=start_time, end_time=end_time)\n",
    "temp_xflux = cc.querying.getvar(exp,'temp_xflux_adv',session,start_time=start_time, end_time=end_time)\n",
    "\n",
    "# select latitude range:\n",
    "temp_yflux = temp_yflux.sel(yu_ocean=lat_range).sel(time=slice(start_time,end_time))\n",
    "temp_xflux = temp_xflux.sel(yt_ocean=lat_range).sel(time=slice(start_time,end_time))\n",
    "\n",
    "# time average and sum in depth:\n",
    "temp_yflux = temp_yflux.mean('time').sum('st_ocean')\n",
    "temp_xflux = temp_xflux.mean('time').sum('st_ocean')\n",
    "\n",
    "temp_yflux = temp_yflux.load()\n",
    "temp_xflux = temp_xflux.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a long term average of vhrho_nt and uhrho_et:\n",
    "\n",
    "outpath = '/g/data/v45/akm157/model_data/access-om2/'+exp+'/Antarctic_cross_slope/uhrho_vhrho_'+time_period+'.nc'\n",
    "# check if already exists:\n",
    "if os.path.exists(outpath):\n",
    "    average_transports = xr.open_dataset(outpath)\n",
    "    # extract arrays from dataset:\n",
    "    uhrho_et = average_transports.uhrho_et\n",
    "    vhrho_nt = average_transports.vhrho_nt\n",
    "else:\n",
    "    vhrho_nt = cc.querying.getvar(exp,'vhrho_nt',session,start_time=start_time, end_time=end_time)\n",
    "    uhrho_et = cc.querying.getvar(exp,'uhrho_et',session,start_time=start_time, end_time=end_time)\n",
    "\n",
    "    vhrho_nt = vhrho_nt.sel(yt_ocean=lat_range).sel(time=slice(start_time,end_time))\n",
    "    uhrho_et = uhrho_et.sel(yt_ocean=lat_range).sel(time=slice(start_time,end_time))\n",
    "\n",
    "    vhrho_nt = vhrho_nt.mean('time')\n",
    "    uhrho_et = uhrho_et.mean('time')\n",
    "\n",
    "    outpath = '/g/data/v45/akm157/model_data/access-om2/'+exp+'/Antarctic_cross_slope/uhrho_vhrho_'+time_period+'.nc'\n",
    "    ds = xr.Dataset({'vhrho_nt': vhrho_nt,'uhrho_et':uhrho_et})\n",
    "    ds.to_netcdf(outpath)\n",
    "    ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract freezing point heat transport:\n",
    "yt_ocean = cc.querying.getvar('01deg_jra55v13_ryf9091','yt_ocean',session,n=1)\n",
    "dxu = cc.querying.getvar('01deg_jra55v13_ryf9091','dxu',session,n=1)\n",
    "dyt = cc.querying.getvar(exp,'dyt',session,n=1)\n",
    "# give dxu and dyt correct coordinates:\n",
    "dxu.coords['nj'] = yt_ocean.values\n",
    "dxu.coords['ni'] = xt_ocean['xt_ocean'].values\n",
    "dxu = dxu.rename(({'ni':'xt_ocean', 'nj':'yt_ocean'}))\n",
    "dyt.coords['nj'] = yt_ocean.values\n",
    "dyt.coords['ni'] = xt_ocean['xt_ocean'].values\n",
    "dyt = dyt.rename(({'ni':'xt_ocean', 'nj':'yt_ocean'}))\n",
    "# select latitude range:\n",
    "dxu = dxu.sel(yt_ocean=lat_range)\n",
    "dyt = dyt.sel(yt_ocean=lat_range)\n",
    "\n",
    "# Note that in newer mom5 versions this could also be done with ty_trans_int_z, \n",
    "# but there is a problem with this diagnostic in older runs, and even\n",
    "# using ty_trans, there is a slight difference. Not sure why?\n",
    "\n",
    "# Note vhrho_nt is v*dz*1035 and is positioned on north centre edge of t-cell.\n",
    "# sum in depth:\n",
    "vhrho_nt = vhrho_nt.sum('st_ocean')\n",
    "uhrho_et = uhrho_et.sum('st_ocean')\n",
    "# convert to transport:\n",
    "vhrho_nt = vhrho_nt*dxu/rho_0\n",
    "uhrho_et = uhrho_et*dyt/rho_0\n",
    "\n",
    "# overwrite coords, so we can add the freezing point (with uhrho_et and vhrho_nt) without problems:\n",
    "yu_ocean = cc.querying.getvar(exp,'yu_ocean',session,n=1)\n",
    "yu_ocean = yu_ocean.sel(yu_ocean=lat_range)\n",
    "vhrho_nt.coords['yt_ocean'] = yu_ocean.values\n",
    "vhrho_nt = vhrho_nt.rename(({'yt_ocean':'yu_ocean'}))\n",
    "uhrho_et.coords['xt_ocean'] = xu_ocean.values\n",
    "uhrho_et = uhrho_et.rename(({'xt_ocean':'xu_ocean'}))\n",
    "\n",
    "freezing_point_heat_trans_zonal = cp*rho_0*uhrho_et*temp_freezing\n",
    "freezing_point_heat_trans_meridional = cp*rho_0*vhrho_nt*temp_freezing\n",
    "\n",
    "# compare both ways:\n",
    "temp_yflux = temp_yflux - freezing_point_heat_trans_meridional\n",
    "temp_xflux = temp_xflux - freezing_point_heat_trans_zonal\n",
    "\n",
    "temp_yflux = temp_yflux.load()\n",
    "temp_xflux = temp_xflux.load()\n",
    "\n",
    "# multiply by isobath contour masks:\n",
    "temp_yflux_with_mask = temp_yflux*mask_y_transport\n",
    "temp_xflux_with_mask = temp_xflux*mask_x_transport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract heat transport values along isobath contour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply by mask to get correct direction into or out of isobath contour:\n",
    "heat_trans_across_contour = np.zeros((num_points))\n",
    "\n",
    "# locations for zonal transport, already calculated indices above:\n",
    "for count in x_indices:\n",
    "    count = int(count)\n",
    "    jj = int(np.where(mask_x_transport_numbered==count)[0])\n",
    "    ii = int(np.where(mask_x_transport_numbered==count)[1])\n",
    "    heat_trans_across_contour[count-1] += temp_xflux_with_mask[jj,ii].values\n",
    "    \n",
    "# locations for meridional transport, already calculated indices above:\n",
    "for count in y_indices:\n",
    "    count = int(count)\n",
    "    jj = int(np.where(mask_y_transport_numbered==count)[0])\n",
    "    ii = int(np.where(mask_y_transport_numbered==count)[1])\n",
    "    heat_trans_across_contour[count-1] += temp_yflux_with_mask[jj,ii].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert cross-slope heat transport from isobath coordinate to longitude coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to longitude coordinate and average into 3 degree longitude bins:\n",
    "\n",
    "# in degrees:\n",
    "bin_width = 3\n",
    "bin_spacing = 0.25\n",
    "lon_west = -280\n",
    "lon_east = 80\n",
    "\n",
    "# new coordinate and midpoints of longitude bins:\n",
    "full_lon_coord = np.arange(lon_west,lon_east+bin_spacing,bin_spacing)\n",
    "lon_bin_midpoints = np.arange(lon_west+bin_width/2,lon_east-bin_width/2,bin_spacing)\n",
    "n_bin_edges = len(full_lon_coord)\n",
    "\n",
    "# sum into longitude bins:\n",
    "# need to be very careful of loops, we can't just mask over longitude values, but instead pick indices \n",
    "# on the isobath contour and sum continously along contour between defined indices.\n",
    "# (i.e. lon_along_contour is not monotonic)\n",
    "# find points on contour to define edges of longitude bins:\n",
    "bin_edge_indices = np.zeros(n_bin_edges)\n",
    "for lon_bin in range(n_bin_edges-1):\n",
    "    # find first isobath point that has the right longitude:\n",
    "    first_point = np.where(lon_along_contour>=full_lon_coord[lon_bin])[0][0]\n",
    "    # then find all other isobath points with the same longitude as that first point:\n",
    "    same_lon_points = np.where(lon_along_contour==lon_along_contour[first_point])[0]\n",
    "    # we want the most southerly of these points on the same longitude line:\n",
    "    bin_edge_indices[lon_bin] = same_lon_points[np.argmin(lat_along_contour[same_lon_points])]\n",
    "    \n",
    "# define east/west edges:\n",
    "bin_edge_indices = bin_edge_indices.astype(int)\n",
    "bin_edge_indices_west = bin_edge_indices[:-int(bin_width/bin_spacing)-1]\n",
    "bin_edge_indices_east = bin_edge_indices[int(bin_width/bin_spacing):-1]\n",
    "n_bins = len(bin_edge_indices_west)\n",
    "\n",
    "# sum heat transport from isobath coord into new longitude coord:\n",
    "cross_slope_heat_trans = np.zeros(n_bins)\n",
    "for lon_bin in range(n_bins):\n",
    "    heat_trans_this_bin = heat_trans_across_contour[bin_edge_indices_west[lon_bin]:bin_edge_indices_east[lon_bin]]\n",
    "    cross_slope_heat_trans[lon_bin] = np.sum(heat_trans_this_bin)\n",
    "    \n",
    "# find average latitude of each bin, so we can plot back on the isobath:\n",
    "lat_bin_midpoints = np.zeros(n_bins)\n",
    "for lon_bin in range(n_bins):\n",
    "    # find nearest isobath point:\n",
    "    lon_index = np.where(lon_along_contour>=lon_bin_midpoints[lon_bin])[0][0]\n",
    "    lat_bin_midpoints[lon_bin] = lat_along_contour[lon_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zonal heat convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make sure the zonal boundaries here match exactly with the zonal boundaries used for the \n",
    "# longitude averaging above, by using same bin_edge_indices.\n",
    "# Just check if isobath point is on x or y grid. if x, sum up to this point from south.\n",
    "# if on y grid, sum zonal transport on nearest u grid point to west.\n",
    "\n",
    "zonal_heat_trans_west = np.zeros(n_bins)\n",
    "for lon_bin in range(n_bins):\n",
    "    # west limit:\n",
    "    # reset these to False:\n",
    "    on_x_grid = False\n",
    "    on_y_grid = False\n",
    "    # mask_x_transport_numbered etc indexing starts from 1 not 0, so add 1:\n",
    "    isobath_west_index = int(bin_edge_indices_west[lon_bin]+1)\n",
    "    # check if the point is on the x or y transport grid:\n",
    "    if len(np.where(mask_x_transport_numbered==isobath_west_index)[0])>0:\n",
    "        on_x_grid = True\n",
    "        jj = int(np.where(mask_x_transport_numbered==isobath_west_index)[0])\n",
    "        ii = int(np.where(mask_x_transport_numbered==isobath_west_index)[1])\n",
    "    elif len(np.where(mask_y_transport_numbered==isobath_west_index)[0])>0:\n",
    "        on_y_grid = True\n",
    "        jj = int(np.where(mask_y_transport_numbered==isobath_west_index)[0])\n",
    "        ii = int(np.where(mask_y_transport_numbered==isobath_west_index)[1])\n",
    "    if on_x_grid == True:\n",
    "        zonal_heat_trans_west[lon_bin] = np.sum(temp_xflux[:jj,ii])\n",
    "    # in this case we want transport half a grid point to the west:\n",
    "    elif on_y_grid == True:\n",
    "        # careful if ii=0, then we need heat trans from lon=80, because at limit of zonal grid\n",
    "        if ii==0:\n",
    "            zonal_heat_trans_west[lon_bin] = np.sum(temp_xflux[:jj+1,-1])\n",
    "        else:\n",
    "            zonal_heat_trans_west[lon_bin] = np.sum(temp_xflux[:jj+1,ii-1])\n",
    "\n",
    "zonal_heat_trans_east = np.zeros(n_bins)\n",
    "for lon_bin in range(n_bins):\n",
    "    # east limit:\n",
    "    # reset these to False:\n",
    "    on_x_grid = False\n",
    "    on_y_grid = False\n",
    "    # mask_x_transport_numbered etc indexing starts from 1 not 0, so add 1:\n",
    "    isobath_east_index = int(bin_edge_indices_east[lon_bin]+1)\n",
    "    # check if the point is on the x or y transport grid:\n",
    "    if len(np.where(mask_x_transport_numbered==isobath_east_index)[0])>0:\n",
    "        on_x_grid = True\n",
    "        jj = int(np.where(mask_x_transport_numbered==isobath_east_index)[0])\n",
    "        ii = int(np.where(mask_x_transport_numbered==isobath_east_index)[1])\n",
    "    elif len(np.where(mask_y_transport_numbered==isobath_east_index)[0])>0:\n",
    "        on_y_grid = True\n",
    "        jj = int(np.where(mask_y_transport_numbered==isobath_east_index)[0])\n",
    "        ii = int(np.where(mask_y_transport_numbered==isobath_east_index)[1])\n",
    "    if on_x_grid == True:\n",
    "        zonal_heat_trans_east[lon_bin] = np.sum(temp_xflux[:jj,ii])\n",
    "    # in this case we want transport half a grid point to the west:\n",
    "    elif on_y_grid == True:\n",
    "        # wrap around to east side of grid:\n",
    "        if ii==0:\n",
    "            zonal_heat_trans_east[lon_bin] = np.sum(temp_xflux[:jj+1,-1])\n",
    "        else:\n",
    "            zonal_heat_trans_east[lon_bin] = np.sum(temp_xflux[:jj+1,ii-1])\n",
    "\n",
    "zonal_convergence = zonal_heat_trans_east - zonal_heat_trans_west"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cross-slope and zonal convergence terms for this isobath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to data arrays, so we can save as netcdf:\n",
    "zonal_convergence = xr.DataArray(zonal_convergence, coords = [('lon_bin_midpoints', lon_bin_midpoints)])\n",
    "cross_slope_heat_trans = xr.DataArray(cross_slope_heat_trans, coords = [('lon_bin_midpoints', lon_bin_midpoints)])\n",
    "heat_trans_across_contour = xr.DataArray(heat_trans_across_contour,coords = [('lon_along_contour',lon_along_contour)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = '/g/data/v45/akm157/model_data/access-om2/'+exp+'/Antarctic_cross_slope/Ant_cross_slope_heat_terms_online_'+str(isobath_depth)+'m_'+time_period+'.nc'\n",
    "ds = xr.Dataset({'zonal_convergence': zonal_convergence,'cross_slope_heat_trans':cross_slope_heat_trans,\n",
    "                'lon_bin_midpoints':lon_bin_midpoints,'lat_bin_midpoints':lat_bin_midpoints,\n",
    "                 'bin_width':bin_width,'bin_spacing':bin_spacing,'heat_trans_across_contour':heat_trans_across_contour,\n",
    "                'lon_along_contour':lon_along_contour})\n",
    "ds.to_netcdf(outpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-22.04]",
   "language": "python",
   "name": "conda-env-analysis3-22.04-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
